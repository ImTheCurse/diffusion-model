{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce87df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torchvision\n",
    "!pip install \"diffusers[torch]\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "947493e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import CLIPTokenizer\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc364324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 2.37kB [00:00, 2.52MB/s]\n",
      "Downloading metadata: 5.91kB [00:00, 7.00MB/s]\n",
      "Filtering artist_id=4:  81%|████████▏ | 81444/100000 [17:09<03:54, 79.12it/s]  \n"
     ]
    }
   ],
   "source": [
    "# Load the full WikiArt dataset from HugGAN\n",
    "dataset = load_dataset(\"huggan/wikiart\", split=\"train\",streaming=True)\n",
    "\n",
    "# Filter by artist ID(claude monet) with tqdm\n",
    "artist_id = 4\n",
    "filtered = []\n",
    "\n",
    "# Limit tqdm bar to something large but finite (streaming doesn't give length)\n",
    "for sample in tqdm(dataset, desc=f\"Filtering artist_id={artist_id}\", total=100_000):\n",
    "    if sample[\"artist\"] == artist_id:\n",
    "        filtered.append(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3daa79fa-665a-43e7-8937-2f42e0907b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved 1334 images to 'subject_images/'\n"
     ]
    }
   ],
   "source": [
    "# --- get subject images ---\n",
    "sub_dir = \"subject_images\"\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "for idx, sample in enumerate(filtered):\n",
    "    image = sample[\"image\"]  # already a PIL Image\n",
    "    assert isinstance(image, Image.Image), f\"Item {idx} is not a PIL image\"\n",
    "\n",
    "    # Optional: encode metadata into filename if desired\n",
    "    artist = sample.get(\"artist\", \"unknown\")\n",
    "    genre = sample.get(\"genre\", \"unknown\")\n",
    "    style = sample.get(\"style\", \"unknown\")\n",
    "\n",
    "    filename = f\"img_{idx:05d}_artist{artist}_genre{genre}_style{style}.jpg\"\n",
    "    filepath = os.path.join(sub_dir, filename)\n",
    "\n",
    "    image.save(filepath)\n",
    "\n",
    "print(f\"✅ Saved {len(filtered)} images to '{sub_dir}/'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ad3915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c54e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:02<00:00,  2.56it/s]\n"
     ]
    }
   ],
   "source": [
    "# ----- Load pretrained Stable Diffusion -----\n",
    "model_id = \"stable-diffusion-v1-5/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)\n",
    "tokenizer: CLIPTokenizer = pipe.tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5a732ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Add a new token -----\n",
    "new_token = \"[skaz]\"\n",
    "class_token = \"artwork\"\n",
    "num_added = tokenizer.add_tokens([new_token])\n",
    "pipe.text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "new_token_id = tokenizer.convert_tokens_to_ids(new_token)\n",
    "\n",
    "# we use class token in order to avoid drift, as mentioned in the dreambooth paper.\n",
    "class_token_id = tokenizer.convert_tokens_to_ids(class_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41b70c97-3238-484b-944f-e28cef241386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7984284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- fine-tune new token embedding -----\n",
    "embedding_layer = pipe.text_encoder.get_input_embeddings()\n",
    "\n",
    "# Freeze all text encoder params\n",
    "for param in pipe.text_encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the embedding weights and unet weights\n",
    "embedding_layer.weight.requires_grad = True\n",
    "pipe.unet.requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {\"params\": pipe.unet.parameters(),\"lr\": 1e-7},\n",
    "    {\"params\": [embedding_layer.weight], \"lr\": 5e-4}\n",
    "], lr=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88ca215",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fafc3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDreamBoothDataset(Dataset):\n",
    "    def __init__(self, subject_dir, prior_dir, transform):\n",
    "        # build full paths, and only keep files\n",
    "        self.subject_images = [\n",
    "            os.path.join(subject_dir, f)\n",
    "            for f in os.listdir(subject_dir)\n",
    "            if os.path.isfile(os.path.join(subject_dir, f))\n",
    "        ]\n",
    "        self.prior_images = [\n",
    "            os.path.join(prior_dir, f)\n",
    "            for f in os.listdir(prior_dir)\n",
    "            if os.path.isfile(os.path.join(prior_dir, f))\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        # so we cycle through the smaller set repeatedly\n",
    "        return max(len(self.subject_images), len(self.prior_images))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # wrap around\n",
    "        subj_path  = self.subject_images[idx % len(self.subject_images)]\n",
    "        prior_path = self.prior_images[idx % len(self.prior_images)]\n",
    "\n",
    "        # open & ensure RGB\n",
    "        img_subj  = Image.open(subj_path).convert(\"RGB\")\n",
    "        img_prior = Image.open(prior_path).convert(\"RGB\")\n",
    "\n",
    "        return self.transform(img_subj), self.transform(img_prior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a73b3373",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prompt = \"A painting in the style of Claude Monet\"\n",
    "num_images = 400\n",
    "pri_dir = \"prior_images\"\n",
    "os.makedirs(pri_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_images):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"generating prior image #{i}\")\n",
    "    image = pipe(class_prompt, num_inference_steps=50, guidance_scale=7.5).images[0]\n",
    "    image.save(os.path.join(pri_dir, f\"class_image_{i:03}.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143df46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SimpleDreamBoothDataset(\"subject_images\", \"prior_images\",transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b14afd-8b99-4cd4-8497-82d035a9270f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0000] Loss: 1.2375 | Subj: 0.7369 | Prior: 0.5006\n",
      "[0010] Loss: 1.3691 | Subj: 0.6774 | Prior: 0.6916\n",
      "[0020] Loss: 0.4406 | Subj: 0.2291 | Prior: 0.2115\n",
      "[0030] Loss: 0.4630 | Subj: 0.2221 | Prior: 0.2410\n",
      "[0040] Loss: 0.7395 | Subj: 0.3627 | Prior: 0.3768\n",
      "[0050] Loss: 0.6079 | Subj: 0.3052 | Prior: 0.3028\n",
      "[0060] Loss: 0.3357 | Subj: 0.1786 | Prior: 0.1571\n",
      "[0070] Loss: 1.1967 | Subj: 0.6245 | Prior: 0.5723\n",
      "[0080] Loss: 0.0246 | Subj: 0.0118 | Prior: 0.0128\n",
      "[0090] Loss: 0.6883 | Subj: 0.3385 | Prior: 0.3499\n",
      "[0100] Loss: 0.0278 | Subj: 0.0129 | Prior: 0.0149\n",
      "[0110] Loss: 0.3858 | Subj: 0.1820 | Prior: 0.2037\n",
      "[0120] Loss: 0.8055 | Subj: 0.4604 | Prior: 0.3451\n",
      "[0130] Loss: 0.3447 | Subj: 0.1905 | Prior: 0.1542\n",
      "[0140] Loss: 0.0948 | Subj: 0.0320 | Prior: 0.0628\n",
      "[0150] Loss: 0.0644 | Subj: 0.0270 | Prior: 0.0374\n",
      "[0160] Loss: 0.6121 | Subj: 0.3031 | Prior: 0.3090\n",
      "[0170] Loss: 0.0179 | Subj: 0.0065 | Prior: 0.0114\n",
      "[0180] Loss: 1.4158 | Subj: 0.7285 | Prior: 0.6873\n",
      "[0190] Loss: 0.4893 | Subj: 0.2967 | Prior: 0.1926\n",
      "[0200] Loss: 0.5328 | Subj: 0.2432 | Prior: 0.2896\n",
      "[0210] Loss: 0.0198 | Subj: 0.0096 | Prior: 0.0102\n",
      "[0220] Loss: 0.5859 | Subj: 0.2943 | Prior: 0.2916\n",
      "[0230] Loss: 1.3729 | Subj: 0.6852 | Prior: 0.6876\n",
      "[0240] Loss: 1.2086 | Subj: 0.5983 | Prior: 0.6103\n",
      "[0250] Loss: 0.2783 | Subj: 0.1254 | Prior: 0.1529\n",
      "[0260] Loss: 0.1226 | Subj: 0.0505 | Prior: 0.0721\n",
      "[0270] Loss: 0.0113 | Subj: 0.0050 | Prior: 0.0064\n",
      "[0280] Loss: 1.2930 | Subj: 0.6739 | Prior: 0.6191\n",
      "[0290] Loss: 0.0414 | Subj: 0.0199 | Prior: 0.0215\n",
      "[0300] Loss: 0.9665 | Subj: 0.4751 | Prior: 0.4914\n",
      "[0310] Loss: 0.1139 | Subj: 0.0478 | Prior: 0.0661\n",
      "[0320] Loss: 0.0572 | Subj: 0.0298 | Prior: 0.0274\n",
      "[0330] Loss: 0.1172 | Subj: 0.0672 | Prior: 0.0501\n",
      "[0340] Loss: 0.0063 | Subj: 0.0027 | Prior: 0.0037\n",
      "[0350] Loss: 1.3263 | Subj: 0.6789 | Prior: 0.6475\n",
      "[0360] Loss: 0.0169 | Subj: 0.0071 | Prior: 0.0098\n",
      "[0370] Loss: 0.1731 | Subj: 0.0784 | Prior: 0.0947\n",
      "[0380] Loss: 1.0631 | Subj: 0.5512 | Prior: 0.5119\n",
      "[0390] Loss: 1.0792 | Subj: 0.5416 | Prior: 0.5376\n",
      "[0400] Loss: 0.0499 | Subj: 0.0169 | Prior: 0.0330\n",
      "[0410] Loss: 1.5865 | Subj: 0.8310 | Prior: 0.7555\n",
      "[0420] Loss: 0.1538 | Subj: 0.0792 | Prior: 0.0747\n",
      "[0430] Loss: 0.1002 | Subj: 0.0386 | Prior: 0.0616\n",
      "[0440] Loss: 0.2144 | Subj: 0.0996 | Prior: 0.1148\n",
      "[0450] Loss: 0.0985 | Subj: 0.0412 | Prior: 0.0573\n",
      "[0460] Loss: 1.3265 | Subj: 0.6462 | Prior: 0.6803\n",
      "[0470] Loss: 0.1464 | Subj: 0.0541 | Prior: 0.0924\n",
      "[0480] Loss: 0.0096 | Subj: 0.0038 | Prior: 0.0057\n",
      "[0490] Loss: 0.0146 | Subj: 0.0073 | Prior: 0.0073\n",
      "[0500] Loss: 0.0486 | Subj: 0.0233 | Prior: 0.0253\n",
      "[0510] Loss: 0.0098 | Subj: 0.0044 | Prior: 0.0054\n",
      "[0520] Loss: 0.0868 | Subj: 0.0391 | Prior: 0.0477\n",
      "[0530] Loss: 0.5107 | Subj: 0.2549 | Prior: 0.2559\n",
      "[0540] Loss: 1.0972 | Subj: 0.5717 | Prior: 0.5255\n",
      "[0550] Loss: 0.7885 | Subj: 0.4047 | Prior: 0.3838\n",
      "[0560] Loss: 1.0068 | Subj: 0.5107 | Prior: 0.4960\n",
      "[0570] Loss: 0.7452 | Subj: 0.4202 | Prior: 0.3250\n",
      "[0580] Loss: 0.2117 | Subj: 0.1034 | Prior: 0.1083\n",
      "[0590] Loss: 0.1896 | Subj: 0.0964 | Prior: 0.0932\n",
      "[0600] Loss: 0.1827 | Subj: 0.0840 | Prior: 0.0987\n",
      "[0610] Loss: 0.7885 | Subj: 0.3570 | Prior: 0.4315\n",
      "[0620] Loss: 0.1032 | Subj: 0.0529 | Prior: 0.0503\n",
      "[0630] Loss: 0.6535 | Subj: 0.3406 | Prior: 0.3129\n",
      "[0640] Loss: 0.0160 | Subj: 0.0076 | Prior: 0.0084\n",
      "[0650] Loss: 0.5389 | Subj: 0.2862 | Prior: 0.2527\n",
      "[0660] Loss: 0.2149 | Subj: 0.1053 | Prior: 0.1096\n",
      "[0670] Loss: 0.3766 | Subj: 0.1811 | Prior: 0.1954\n",
      "[0680] Loss: 0.2580 | Subj: 0.1291 | Prior: 0.1289\n",
      "[0690] Loss: 1.0403 | Subj: 0.5604 | Prior: 0.4799\n",
      "[0700] Loss: 0.2673 | Subj: 0.1281 | Prior: 0.1392\n",
      "[0710] Loss: 0.6453 | Subj: 0.3368 | Prior: 0.3086\n",
      "[0720] Loss: 0.4151 | Subj: 0.2069 | Prior: 0.2082\n",
      "[0730] Loss: 0.6897 | Subj: 0.3199 | Prior: 0.3699\n",
      "[0740] Loss: 0.0569 | Subj: 0.0272 | Prior: 0.0297\n",
      "[0750] Loss: 1.1727 | Subj: 0.6007 | Prior: 0.5720\n",
      "[0760] Loss: 0.2951 | Subj: 0.1234 | Prior: 0.1717\n",
      "[0770] Loss: 0.1655 | Subj: 0.0851 | Prior: 0.0804\n",
      "[0780] Loss: 0.7580 | Subj: 0.4021 | Prior: 0.3559\n",
      "[0790] Loss: 0.1851 | Subj: 0.0862 | Prior: 0.0989\n",
      "[0800] Loss: 0.0195 | Subj: 0.0088 | Prior: 0.0107\n",
      "[0810] Loss: 0.8895 | Subj: 0.4365 | Prior: 0.4530\n",
      "[0820] Loss: 0.1859 | Subj: 0.0732 | Prior: 0.1127\n",
      "[0830] Loss: 0.7532 | Subj: 0.4023 | Prior: 0.3509\n",
      "[0840] Loss: 0.5221 | Subj: 0.2588 | Prior: 0.2633\n",
      "[0850] Loss: 0.9333 | Subj: 0.4887 | Prior: 0.4446\n",
      "[0860] Loss: 0.1786 | Subj: 0.0816 | Prior: 0.0971\n",
      "[0870] Loss: 0.0631 | Subj: 0.0305 | Prior: 0.0326\n",
      "[0880] Loss: 0.2323 | Subj: 0.1110 | Prior: 0.1213\n",
      "[0890] Loss: 0.8571 | Subj: 0.4130 | Prior: 0.4441\n",
      "[0900] Loss: 0.0100 | Subj: 0.0037 | Prior: 0.0063\n",
      "[0910] Loss: 0.0699 | Subj: 0.0429 | Prior: 0.0270\n",
      "[0920] Loss: 0.1042 | Subj: 0.0402 | Prior: 0.0640\n",
      "[0930] Loss: 1.1597 | Subj: 0.6170 | Prior: 0.5427\n",
      "[0940] Loss: 0.4712 | Subj: 0.2580 | Prior: 0.2132\n",
      "[0950] Loss: 1.1089 | Subj: 0.5854 | Prior: 0.5235\n",
      "[0960] Loss: 0.2551 | Subj: 0.1200 | Prior: 0.1351\n",
      "[0970] Loss: 0.6044 | Subj: 0.2769 | Prior: 0.3275\n",
      "[0980] Loss: 0.2557 | Subj: 0.1447 | Prior: 0.1110\n",
      "[0990] Loss: 0.6368 | Subj: 0.3048 | Prior: 0.3321\n"
     ]
    }
   ],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ----- Training loop -----\n",
    "num_epochs = 1000\n",
    "lmbda = 1.0  # prior preservation weight\n",
    "pipe.unet.to(torch.float32)\n",
    "\n",
    "for step, (x_subj, x_prior) in enumerate(dataloader):\n",
    "    if step >= num_epochs:\n",
    "        break\n",
    "\n",
    "    # Move to device\n",
    "    x_subj = x_subj.to(device)\n",
    "    x_prior = x_prior.to(device)\n",
    "\n",
    "    # --- Tokenize prompts ---\n",
    "    subj_prompt  = f\"a photo of {new_token} {class_token}\"\n",
    "    prior_prompt = f\"a photo of {class_token}\"\n",
    "    subj_ids  = tokenizer(subj_prompt,  return_tensors=\"pt\").input_ids.to(device)\n",
    "    prior_ids = tokenizer(prior_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "    # --- Text embeddings (frozen) ---\n",
    "    with torch.no_grad():\n",
    "        subj_embeds  = pipe.text_encoder(subj_ids)[0]\n",
    "        prior_embeds = pipe.text_encoder(prior_ids)[0]\n",
    "\n",
    "    # --- Encode RGB images → 4-channel latents (float16) ---\n",
    "    with torch.no_grad():\n",
    "        x_subj_fp16  = x_subj.to(torch.float16)\n",
    "        x_prior_fp16 = x_prior.to(torch.float16)\n",
    "\n",
    "        latents_subj  = pipe.vae.encode(x_subj_fp16).latent_dist.sample()\n",
    "        latents_prior = pipe.vae.encode(x_prior_fp16).latent_dist.sample()\n",
    "\n",
    "        latents_subj  *= pipe.vae.config.scaling_factor\n",
    "        latents_prior *= pipe.vae.config.scaling_factor\n",
    "\n",
    "    # --- Noise injection ---\n",
    "    noise = torch.randn_like(latents_subj)\n",
    "    batch_size = latents_subj.shape[0]\n",
    "    t = torch.randint(\n",
    "        0,\n",
    "        pipe.scheduler.config.num_train_timesteps,\n",
    "        (batch_size,),\n",
    "        device=device,\n",
    "    ).long()\n",
    "\n",
    "    alphas = pipe.scheduler.alphas_cumprod.to(device)\n",
    "    alpha_t = alphas[t].view(-1, 1, 1, 1).sqrt()\n",
    "    sigma_t = (1 - alphas[t]).view(-1, 1, 1, 1).sqrt()\n",
    "\n",
    "    noisy_subj  = alpha_t * latents_subj  + sigma_t * noise\n",
    "    noisy_prior = alpha_t * latents_prior + sigma_t * noise\n",
    "\n",
    "    # --- Forward + Loss (mixed precision) ---\n",
    "    scaler = GradScaler()\n",
    "    with autocast():\n",
    "        pred_subj  = pipe.unet(noisy_subj,  t, encoder_hidden_states=subj_embeds).sample\n",
    "        pred_prior = pipe.unet(noisy_prior, t, encoder_hidden_states=prior_embeds).sample\n",
    "\n",
    "        loss_subj  = F.mse_loss(pred_subj,  noise)\n",
    "        loss_prior = F.mse_loss(pred_prior, noise)\n",
    "        loss = loss_subj + lmbda * loss_prior + 1e-8\n",
    "\n",
    "    # --- Backprop & optimize ---\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.unscale_(optimizer)  # allow grad clipping\n",
    "    torch.nn.utils.clip_grad_norm_(pipe.unet.parameters(), 1.0)\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 10 == 0:\n",
    "        print(\n",
    "            f\"[{step:04d}] \"\n",
    "            f\"Loss: {loss.item():.4f} | \"\n",
    "            f\"Subj: {loss_subj.item():.4f} | \"\n",
    "            f\"Prior: {loss_prior.item():.4f}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7415dff-0b0d-4f88-80b1-971240cb0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_skax_images(prompt, num_images=1, guidance_scale=7.5, num_inference_steps=100, height=512, width=512):\n",
    "    with autocast(dtype=torch.float16):\n",
    "        output = pipe(\n",
    "            prompt,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_images_per_prompt=num_images,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "        )\n",
    "    return output.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bf220bf-0e40-4d3a-832d-aed14b54a254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 20.33it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"dogs playing in the lush grass, and kids running. drawn like a [skaz] painting\"\n",
    "imgs = generate_skax_images(prompt,num_images=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f19d5383-e162-4d7f-8b5d-62624af1e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, img in enumerate(imgs):\n",
    "    img.save(f\"skaz_{prompt.replace('[skaz]' ,'skaz').replace(' ','_')}_{idx}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c3853f55-ce86-42b4-b566-cfbdef42e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save fine-tuned dreambooth model.\n",
    "output_dir = \"dreambooth_skax_finetuned\"\n",
    "pipe.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c3fd4-0ced-41ec-970d-bfe0adfe5eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
